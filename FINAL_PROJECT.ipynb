{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jack-ki1/MARTIAL_JENGA_PROJECT_AI_ENGINEERING/blob/main/FINAL_PROJECT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f0d1288-d981-43cc-802e-7ec0527618ae",
      "metadata": {
        "id": "3f0d1288-d981-43cc-802e-7ec0527618ae"
      },
      "source": [
        "Phase 1: Problem Definition and Data Collection\n",
        "\n",
        "Define the Goal: Clearly state the objective: to build a regression model that predicts the numerical value of a customer's purchase amount (in USD)\n",
        "based on their demographic and behavioral data.\n",
        "\n",
        "Collect Data: Download the \"Customer Shopping Trends Dataset\" from Kaggle and load it into your chosen environment (e.g., Python with Pandas).\n",
        "\n",
        "Phase 2: Exploratory Data Analysis (EDA) and Data Preparation\n",
        "\n",
        "Understand the Data: Examine the dataset's structure, identify data types (numerical, categorical), look for missing values, and analyze the distribution of key variables like \"Age,\" \"Annual Income,\" and the target variable \"Purchase Amount (USD)\".\n",
        "\n",
        "Handle Missing Values: Decide how to manage any missing or null data points. This might involve removing rows with missing values or filling them in (imputation) based on statistical measures (e.g., mean, median) or other predictive methods.\n",
        "\n",
        "Encode Categorical Variables: Machine learning models typically require numerical input. Convert categorical features like \"Gender,\" \"Item Purchased,\" \"Category,\" \"Payment Method,\" \"Color,\" \"Season,\" and \"Location\" into a numerical format using techniques like one-hot encoding or label encoding.\n",
        "\n",
        "Feature Engineering (Optional but Recommended): Create new, more informative features from existing ones if possible (e.g., a \"Total Purchases per Year\" metric if raw dates are available).\n",
        "\n",
        "Split the Data: Divide your dataset into two or three parts: a training set (for teaching the model), a validation set (for tuning the model), and a test set (for evaluating the final, trained model on unseen data).\n",
        "\n",
        "Phase 3: Model Building and Training\n",
        "Select a Model: Choose appropriate regression algorithms suitable for predicting continuous numerical values. Popular choices for this type of problem include:\n",
        "\n",
        "Linear Regression (simple baseline model)\n",
        "\n",
        "Random Forest Regressor (generally high performance)\n",
        "\n",
        "XGBoost Regressor (often wins competitions)\n",
        "\n",
        "Support Vector Regressor (SVR)\n",
        "\n",
        "Train the Model: Use the training data to train the selected algorithm(s) to find patterns and relationships between the input features and the target variable \"Purchase Amount (USD)\".\n",
        "\n",
        "Phase 4: Model Evaluation and Tuning\n",
        "\n",
        "Evaluate Performance: Assess how well your model makes predictions using appropriate regression metrics. Key metrics include:\n",
        "\n",
        "Root Mean Square Error (RMSE): Measures the average difference between the predicted and actual amounts; lower is better.\n",
        "\n",
        "Mean Absolute Error (MAE): Another measure of average error, often easier to interpret in the original units (USD).\n",
        "\n",
        "R-squared (R²): Indicates the proportion of the variance in the target variable that is predictable from the features.\n",
        "\n",
        "Tune Hyperparameters: Adjust the internal settings (hyperparameters) of your chosen model(s) to optimize performance.\n",
        "\n",
        "Techniques like cross-validation can ensure robustness and prevent overfitting.\n",
        "\n",
        "Phase 5: Deployment and Further Steps\n",
        "\n",
        "Make Predictions: Once you are satisfied with your model's performance, use it to make predictions on your held-out test set or new, unseen customer data.\n",
        "\n",
        "Interpret and Deploy: Understand the insights gained from the model (e.g., which features most influence purchase amount). You can then integrate the model into a practical application or a business decision-making process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da696120-f379-44dc-89fe-96b9bd13c5ac",
      "metadata": {
        "id": "da696120-f379-44dc-89fe-96b9bd13c5ac"
      },
      "source": [
        "## DEFIINE TASK:\n",
        "\n",
        "### Building a regression model to predict the numerical value of a customer's purchase amount (in USD) based on their demographic and behavioral data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d3c54284-47ba-415a-bc28-0a38077c40c0",
      "metadata": {
        "id": "d3c54284-47ba-415a-bc28-0a38077c40c0"
      },
      "outputs": [],
      "source": [
        "#load librairies\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6116deb2-db23-4958-9dba-869f322c0a4d",
      "metadata": {
        "id": "6116deb2-db23-4958-9dba-869f322c0a4d"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2622f091-ef2a-479c-a580-48fd9c19c8c7",
      "metadata": {
        "id": "2622f091-ef2a-479c-a580-48fd9c19c8c7",
        "outputId": "df99898f-133c-40c5-9567-289cbcf7e565",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'shopping_trends.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-577335771.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shopping_trends.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"A look IN THE SHAPE OF THE DATASET:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"A SAMPLE LOOK INTO THE DATASET:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'shopping_trends.csv'"
          ]
        }
      ],
      "source": [
        "df2=pd.read_csv(\"shopping_trends.csv\")\n",
        "df=df2.copy()\n",
        "\n",
        "display(\"A look IN THE SHAPE OF THE DATASET:\",df.shape)\n",
        "display(\"A SAMPLE LOOK INTO THE DATASET:\",df.sample(20))\n",
        "display(\"A DESCRIPTION OF THE DATASET:\",df.describe())\n",
        "display(\"INFORMATION ON THE DATASET:\",df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6ddef5c-60ac-471f-bc60-e44efc7a9109",
      "metadata": {
        "id": "d6ddef5c-60ac-471f-bc60-e44efc7a9109"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a900f900-c07d-45bb-af64-74eecd5bd8d2",
      "metadata": {
        "id": "a900f900-c07d-45bb-af64-74eecd5bd8d2"
      },
      "outputs": [],
      "source": [
        "df.drop(['Item Purchased','Location', 'Size', 'Color', 'Season','Shipping Type', 'Discount Applied', 'Promo Code Used','Preferred Payment Method'], axis=1, inplace=True)\n",
        "display(\"SHAPE OF THE DATASET:\",df.shape)\n",
        "display(\"INFORMATION ON THE DATASET:\",df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0886ff3f-a3b6-4415-b144-75f8e37ebaa3",
      "metadata": {
        "id": "0886ff3f-a3b6-4415-b144-75f8e37ebaa3"
      },
      "outputs": [],
      "source": [
        "#using one hot encoder to encode non-numeric columns\n",
        "#from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
        "#from sklearn.compose import ColumnTransformer\n",
        "#from sklearn.pipeline import Pipeline\n",
        "\n",
        "# --- 2. Define Columns by Encoding Type ---\n",
        "\n",
        "# Columns for One-Hot Encoding (Nominal, Unordered)\n",
        "one_hot_cols = ['Gender', 'Category', 'Subscription Status', 'Payment Method']\n",
        "\n",
        "# Column for Ordinal Encoding (Ordered)\n",
        "ordinal_col = ['Frequency of Purchases']\n",
        "\n",
        "# Numerical columns (to be passed through without encoding)\n",
        "numerical_cols = ['Customer ID','Age', 'Previous Purchases', 'Review Rating', 'Purchase Amount (USD)'] # Include target variable here to pass through initially\n",
        "\n",
        "# --- 3. Define the Ordinal Categories in the Correct Order ---\n",
        "# This step is crucial to maintain the ranking\n",
        "frequency_order = [\n",
        "    'Rarely',\n",
        "    'Annually',\n",
        "    'Every 3 Months',\n",
        "    'Quarterly',\n",
        "    'Bi-Weekly',\n",
        "    'Fortnightly',\n",
        "    'Monthly',\n",
        "    'Weekly'\n",
        "]\n",
        "\n",
        "ordinal_categories = [frequency_order]\n",
        "\n",
        "# --- 4. Create Preprocessing Pipelines ---\n",
        "\n",
        "# One-hot encoder pipeline\n",
        "one_hot_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "\n",
        "# Ordinal encoder pipeline\n",
        "ordinal_transformer = OrdinalEncoder(categories=ordinal_categories)\n",
        "\n",
        "# --- 5. Combine Transformers using ColumnTransformer ---\n",
        "# This applies the correct transformation to each column group simultaneously\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('onehot', one_hot_transformer, one_hot_cols),\n",
        "        ('ordinal', ordinal_transformer, ordinal_col),\n",
        "        ('numeric', 'passthrough', numerical_cols) # Keep numerical columns as they are\n",
        "    ],\n",
        "    remainder='drop' # Drops any columns not specified\n",
        ")\n",
        "\n",
        "# --- 6. Apply the transformations and create the final encoded DataFrame ---\n",
        "\n",
        "# Fit and transform the data\n",
        "df_encoded_np = preprocessor.fit_transform(df)\n",
        "\n",
        "# Get the new column names for the one-hot encoded features\n",
        "one_hot_feature_names = preprocessor.named_transformers_['onehot'].get_feature_names_out(one_hot_cols)\n",
        "\n",
        "# Combine all new column names in the correct order\n",
        "all_feature_names = list(one_hot_feature_names) + ordinal_col + numerical_cols\n",
        "\n",
        "# Convert the resulting numpy array back into a pandas DataFrame\n",
        "df_encoded = pd.DataFrame(df_encoded_np, columns=all_feature_names)\n",
        "\n",
        "# --- 7. Display the result ---\n",
        "print(df_encoded.head())\n",
        "print(\"\\nEncoded DataFrame shape:\", df_encoded.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96862a10-0974-4612-b33b-0b1926a892d1",
      "metadata": {
        "id": "96862a10-0974-4612-b33b-0b1926a892d1"
      },
      "outputs": [],
      "source": [
        "# Assuming 'df_encoded' is your final DataFrame from the previous step\n",
        "\n",
        "# Define the target variable (y)\n",
        "y = df_encoded['Purchase Amount (USD)']\n",
        "\n",
        "# Define the features (X) by dropping the target column from the DataFrame\n",
        "X = df_encoded.drop('Purchase Amount (USD)', axis=1)\n",
        "\n",
        "# Verify the shapes of your X and y\n",
        "print(f\"Shape of X (features): {X.shape}\")\n",
        "print(f\"Shape of y (target): {y.shape}\")\n",
        "\n",
        "# View the first few rows of X to confirm the structure\n",
        "print(\"\\nFirst 5 rows of X:\")\n",
        "print(X.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d794c09-b206-4f8d-b188-50f5a0242cbe",
      "metadata": {
        "id": "4d794c09-b206-4f8d-b188-50f5a0242cbe"
      },
      "outputs": [],
      "source": [
        "# Split the data with an 80/20 ratio\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42 )\n",
        "\n",
        "# Print the size of each new set\n",
        "print(f\"\\nTraining set size: {X_train.shape[0]} samples\")\n",
        "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
        "print(f\"y_train size: {y_train.shape[0]}\")\n",
        "print(f\"y_test size: {y_test.shape[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c77d51a-37af-4a2c-b277-c8762a9c0794",
      "metadata": {
        "id": "7c77d51a-37af-4a2c-b277-c8762a9c0794"
      },
      "outputs": [],
      "source": [
        "# Preprocessing: Linear Regression is sensitive to scale → standardize\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b6013ed-ed98-45f7-8deb-e71ac75b40b7",
      "metadata": {
        "id": "7b6013ed-ed98-45f7-8deb-e71ac75b40b7"
      },
      "outputs": [],
      "source": [
        "#use the linearregression model\n",
        "model=LinearRegression()\n",
        "model.fit(X_train_scaled, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "932940f5-c5f7-42c1-993a-12ea50cee82a",
      "metadata": {
        "id": "932940f5-c5f7-42c1-993a-12ea50cee82a"
      },
      "outputs": [],
      "source": [
        "#MAKE PREDICTIONS\n",
        "y_train_pred = model.predict(X_train_scaled)\n",
        "y_test_pred = model.predict(X_test_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fca6becb-bcbd-4e36-8cf9-81013b01c78d",
      "metadata": {
        "id": "fca6becb-bcbd-4e36-8cf9-81013b01c78d"
      },
      "outputs": [],
      "source": [
        "#--- MODEL EVALUATION --\n",
        "# Calculate key regression metrics on both train and test sets\n",
        " # This helps diagnose overfitting (large gap between train and test performance)\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "train_rmse = np.sqrt(train_mse)\n",
        "test_rmse = np.sqrt(test_mse)\n",
        "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "\n",
        "print(\"=== LINEAR REGRESSION PERFORMANCE ===\")\n",
        "print(f\"Train RMSE: ${train_rmse * 100_000:,.2f} | Test RMSE: ${test_rmse *\n",
        "100_000:,.2f}\")\n",
        "print(f\"Train MAE:  ${train_mae * 100_000:,.2f} | Test MAE:  ${test_mae *\n",
        "100_000:,.2f}\")\n",
        "print(f\"Train R²:   {train_r2:.4f} | Test R²: {test_r2:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35f47d15-f0e6-4e8b-be18-240b8e87ebd1",
      "metadata": {
        "id": "35f47d15-f0e6-4e8b-be18-240b8e87ebd1"
      },
      "outputs": [],
      "source": [
        "# --- 7. MODEL INTERPRETATION --\n",
        "# Extract feature names and their corresponding coefficients (weights)\n",
        "feature_names = housing.feature_names\n",
        "coefficients = model.coef_\n",
        "2025-11-13\n",
        " {test_r2:.4f}\")\n",
        "# Create a DataFrame for easy sorting and visualization\n",
        "coef_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
        "coef_df = coef_df.sort_values(by='Coefficient', key=abs, ascending=False)  # Sort\n",
        "by absolute magnitude\n",
        "# Plot the feature importances (coefficients)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(coef_df['Feature'], coef_df['Coefficient'], color='skyblue')\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.title('Linear Regression: Feature Coefficients (Impact on House Price)')\n",
        "plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)  # Vertical line at\n",
        "zero\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ec1b989-c378-4a0b-ab67-f16b9d1c1144",
      "metadata": {
        "id": "9ec1b989-c378-4a0b-ab67-f16b9d1c1144"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66f258ef-e0fe-4aa0-a2f4-2f9fe6b56702",
      "metadata": {
        "id": "66f258ef-e0fe-4aa0-a2f4-2f9fe6b56702"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.12",
      "language": "python",
      "name": "python3.12"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}